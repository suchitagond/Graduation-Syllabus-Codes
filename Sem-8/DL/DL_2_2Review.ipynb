{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae457793-932f-4dc9-9be3-5c98e736c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Regular expressions for text cleaning\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # Optional: for stemming\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib # To save/load models and vectorizer (optional)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3eddf1-7fd8-4911-86ef-c4c9f02151d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "\n",
      "First 5 rows:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "\n",
      "Sentiment value counts (1: positive, 0: negative):\n",
      "sentiment\n",
      "1    25000\n",
      "0    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"imdb.csv\")\n",
    "# Display basic info and first few rows\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Map sentiment labels to numerical values\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "print(\"\\nSentiment value counts (1: positive, 0: negative):\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3df228-02db-41ce-82a4-8ec5c08d3d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text data... (This may take a few minutes)\n",
      "Text preprocessing complete.\n",
      "\n",
      "Example Preprocessing:\n",
      "Original: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me abo...\n",
      "Cleaned: one reviewers mentioned watching oz episode youll hooked right exactly happened methe first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show ...\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Preprocessing Function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# ps = PorterStemmer() # Initialize stemmer if using\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove punctuation and numbers, keep only letters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize (split into words) and remove stop words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Optional: Stemming\n",
    "    # words = [ps.stem(word) for word in words]\n",
    "    # Join words back into a string\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# Apply the preprocessing function to the 'review' column\n",
    "print(\"\\nPreprocessing text data... (This may take a few minutes)\")\n",
    "# Create a new column for cleaned reviews\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "print(\"Text preprocessing complete.\")\n",
    "\n",
    "# Display original vs cleaned review for one example\n",
    "print(\"\\nExample Preprocessing:\")\n",
    "print(\"Original:\", df['review'][0][:200] + \"...\") # Show first 200 chars\n",
    "print(\"Cleaned:\", df['cleaned_review'][0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f29790-2838-4954-abc1-33d000552dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Split:\n",
      "Training set size: 37500 samples\n",
      "Testing set size: 12500 samples\n",
      "\n",
      "Fitting TF-IDF Vectorizer and transforming training data...\n",
      "Transforming test data...\n",
      "TF-IDF transformation complete.\n",
      "Shape of TF-IDF matrix (Train): (37500, 5000)\n",
      "Shape of TF-IDF matrix (Test): (12500, 5000)\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Split Data into Training and Testing Sets\n",
    "X = df['cleaned_review']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "# stratify=y ensures the proportion of positive/negative reviews is similar in train and test sets\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "\n",
    "# 3.2 TF-IDF Vectorization\n",
    "# Initialize TF-IDF Vectorizer\n",
    "# max_features limits the vocabulary size to the most frequent terms, useful for large datasets\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You can tune max_features\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training data\n",
    "print(\"\\nFitting TF-IDF Vectorizer and transforming training data...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the *same* fitted vectorizer\n",
    "print(\"Transforming test data...\")\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF transformation complete.\")\n",
    "print(f\"Shape of TF-IDF matrix (Train): {X_train_tfidf.shape}\") # (num_samples, num_features)\n",
    "print(f\"Shape of TF-IDF matrix (Test): {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd2f1a8-0409-4e86-9565-facc6fb35bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Deep Neural Network model...\n",
      "Epoch 1/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.7096 - loss: 0.6234 - val_accuracy: 0.8781 - val_loss: 0.2976\n",
      "Epoch 2/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8954 - loss: 0.2675 - val_accuracy: 0.8825 - val_loss: 0.2750\n",
      "Epoch 3/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9173 - loss: 0.2140 - val_accuracy: 0.8821 - val_loss: 0.2891\n",
      "Epoch 4/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.9267 - loss: 0.1921 - val_accuracy: 0.8779 - val_loss: 0.3018\n",
      "Epoch 5/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9399 - loss: 0.1682 - val_accuracy: 0.8756 - val_loss: 0.3149\n",
      "Epoch 6/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9528 - loss: 0.1420 - val_accuracy: 0.8745 - val_loss: 0.3338\n",
      "Epoch 7/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9638 - loss: 0.1195 - val_accuracy: 0.8712 - val_loss: 0.3589\n",
      "Epoch 8/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9746 - loss: 0.0916 - val_accuracy: 0.8715 - val_loss: 0.3842\n",
      "Epoch 9/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9841 - loss: 0.0674 - val_accuracy: 0.8700 - val_loss: 0.4186\n",
      "Epoch 10/10\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9900 - loss: 0.0476 - val_accuracy: 0.8683 - val_loss: 0.4631\n",
      "Model training complete.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8703 - loss: 0.4376\n",
      "\n",
      "Test Accuracy: 0.8691\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Define the DNN model\n",
    "dnn_model = Sequential()\n",
    "dnn_model.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
    "dnn_model.add(Dropout(0.3))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dropout(0.3))\n",
    "dnn_model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Step 2: Compile the model\n",
    "dnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 3: Train the model\n",
    "print(\"\\nTraining Deep Neural Network model...\")\n",
    "history = dnn_model.fit(\n",
    "    X_train_tfidf.toarray(), y_train,  # TF-IDF is usually sparse, convert to dense\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Step 4: Evaluate on test set\n",
    "loss, accuracy = dnn_model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b25d67-d54c-4ddc-9f5a-50911915964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating DNN model on the test set...\n",
      "\u001b[1m391/391\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\n",
      "Accuracy: 0.8691\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5395  855]\n",
      " [ 781 5469]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negative (0)       0.87      0.86      0.87      6250\n",
      "Positive (1)       0.86      0.88      0.87      6250\n",
      "\n",
      "    accuracy                           0.87     12500\n",
      "   macro avg       0.87      0.87      0.87     12500\n",
      "weighted avg       0.87      0.87      0.87     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "print(\"\\nEvaluating DNN model on the test set...\")\n",
    "y_pred_prob = dnn_model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Format:\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative (0)', 'Positive (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6362279-faaf-469e-a902-e52c34672422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing on New Reviews ---\n",
      "Cleaned Reviews: ['movie absolutely fantastic acting superb storyline kept engaged throughout', 'waste time plot predictable characters incredibly boring would recommend film', 'okay movie great terrible either good moments overall quite average']\n",
      "Shape of TF-IDF for new reviews: (3, 5000)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "Review: \"This movie was absolutely fantastic! The acting was superb and the storyline kept me engaged through...\"\n",
      "Predicted Sentiment: Positive (1) with confidence: 1.0000\n",
      "\n",
      "Review: \"What a waste of time. The plot was predictable and the characters were incredibly boring. I would no...\"\n",
      "Predicted Sentiment: Negative (0) with confidence: 0.0000\n",
      "\n",
      "Review: \"It was an okay movie, not great but not terrible either. Some good moments but overall quite average...\"\n",
      "Predicted Sentiment: Negative (0) with confidence: 0.2362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Sentiment mapping\n",
    "sentiment_labels = {1: 'Positive', 0: 'Negative'}\n",
    "\n",
    "# New review examples\n",
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the storyline kept me engaged throughout.\",\n",
    "    \"What a waste of time. The plot was predictable and the characters were incredibly boring. I would not recommend this film.\",\n",
    "    \"It was an okay movie, not great but not terrible either. Some good moments but overall quite average.\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Testing on New Reviews ---\")\n",
    "\n",
    "# Step 1: Preprocess the new reviews\n",
    "cleaned_new_reviews = [preprocess_text(review) for review in new_reviews]\n",
    "print(\"Cleaned Reviews:\", cleaned_new_reviews)\n",
    "\n",
    "# Step 2: Transform to TF-IDF (use the same vectorizer you used for training)\n",
    "new_reviews_tfidf = tfidf_vectorizer.transform(cleaned_new_reviews)\n",
    "print(\"Shape of TF-IDF for new reviews:\", new_reviews_tfidf.shape)\n",
    "\n",
    "# Step 3: Predict using the DNN model\n",
    "new_predictions_prob = dnn_model.predict(new_reviews_tfidf.toarray())\n",
    "new_predictions = (new_predictions_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Step 4: Print results\n",
    "for review, pred, prob in zip(new_reviews, new_predictions, new_predictions_prob):\n",
    "    print(f\"\\nReview: \\\"{review[:100]}...\\\"\")\n",
    "    print(f\"Predicted Sentiment: {sentiment_labels[pred]} ({pred}) with confidence: {prob[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6685f45e-d3fc-4711-93a9-c5e4367f2cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "--- Testing on Extra Reviews ---\n",
      "\n",
      "Review: \"Honestly, I fell asleep halfway through. That should tell you enough....\"\n",
      "Predicted Sentiment: ðŸ˜  Negative (0.0004)\n",
      "\n",
      "Review: \"I've never laughed so hard in my life. 10/10 comedy!...\"\n",
      "Predicted Sentiment: ðŸ˜Š Positive (0.9043)\n",
      "\n",
      "Review: \"This was the best worst movie I've ever seen. So bad it's actually good....\"\n",
      "Predicted Sentiment: ðŸ˜  Negative (0.0021)\n",
      "\n",
      "Review: \"Meh. It exists. That's about the best thing I can say....\"\n",
      "Predicted Sentiment: ðŸ˜Š Positive (0.9918)\n",
      "\n",
      "Review: \"Wow. Just wow. I didn't expect much, and yet I was still disappointed....\"\n",
      "Predicted Sentiment: ðŸ˜  Negative (0.0071)\n",
      "\n",
      "Review: \"What a cinematic masterpiece. A pure work of art!...\"\n",
      "Predicted Sentiment: ðŸ˜Š Positive (0.9751)\n",
      "\n",
      "Review: \"Two hours of my life I can never get back. Thanks a lot....\"\n",
      "Predicted Sentiment: ðŸ˜Š Positive (0.9690)\n",
      "\n",
      "Review: \"It was alright, I guess. Not my favorite but watchable....\"\n",
      "Predicted Sentiment: ðŸ˜  Negative (0.0002)\n",
      "\n",
      "Review: \"Acting? What acting? Felt like a school play on a low budget....\"\n",
      "Predicted Sentiment: ðŸ˜  Negative (0.0132)\n",
      "\n",
      "Review: \"Not terrible, not great. Just average in every way....\"\n",
      "Predicted Sentiment: ðŸ˜  Negative (0.0636)\n"
     ]
    }
   ],
   "source": [
    "extra_reviews = [\n",
    "    \"Honestly, I fell asleep halfway through. That should tell you enough.\",\n",
    "    \"I've never laughed so hard in my life. 10/10 comedy!\",\n",
    "    \"This was the best worst movie I've ever seen. So bad it's actually good.\",\n",
    "    \"Meh. It exists. That's about the best thing I can say.\",\n",
    "    \"Wow. Just wow. I didn't expect much, and yet I was still disappointed.\",\n",
    "    \"What a cinematic masterpiece. A pure work of art!\",\n",
    "    \"Two hours of my life I can never get back. Thanks a lot.\",\n",
    "    \"It was alright, I guess. Not my favorite but watchable.\",\n",
    "    \"Acting? What acting? Felt like a school play on a low budget.\",\n",
    "    \"Not terrible, not great. Just average in every way.\"\n",
    "]\n",
    "# Clean and vectorize\n",
    "cleaned_extra_reviews = [preprocess_text(review) for review in extra_reviews]\n",
    "extra_reviews_tfidf = tfidf_vectorizer.transform(cleaned_extra_reviews)\n",
    "\n",
    "# Predict using the DNN model\n",
    "extra_predictions_prob = dnn_model.predict(extra_reviews_tfidf.toarray())\n",
    "extra_predictions = (extra_predictions_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Output predictions\n",
    "print(\"\\n--- Testing on Extra Reviews ---\")\n",
    "for review, pred, prob in zip(extra_reviews, extra_predictions, extra_predictions_prob):\n",
    "    print(f\"\\nReview: \\\"{review[:100]}...\\\"\")\n",
    "    print(f\"Predicted Sentiment: {'ðŸ˜Š Positive' if pred == 1 else 'ðŸ˜  Negative'} ({prob[0]:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
